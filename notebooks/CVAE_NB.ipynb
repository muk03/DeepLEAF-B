{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE Notebook\n",
    "\n",
    "### Implements the training, testing and evaluation scripts for the CVAE architecture in a segemented organised manner.\n",
    "\n",
    "## 1. Importing Relevant Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import glob\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import json\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "import data_utils as du\n",
    "import cvae_tools as nt\n",
    "import model_eval as me\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(me)\n",
    "importlib.reload(nt)\n",
    "importlib.reload(du)\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "s = 42\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.mps.deterministic = True\n",
    "torch.backends.mps.benchmark = False\n",
    "\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.mps.manual_seed(s)\n",
    "random.seed(s)\n",
    "\n",
    "os.chdir('/Users/Mak/Desktop/Imperial College London/Year Four/MSci Project/Codebase/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Variables\n",
    "\n",
    "* `normal_path`: The Directory to stores min-max normalised data and model outputs.\n",
    "\n",
    "* `raw_path`: The directory storing csvs and json files from the MC Simulation Software.\n",
    "\n",
    "    * CSV: Event-wise observables data - in the order: q2, cos_theta_l, cos_theta_d, phi\n",
    "\n",
    "    * JSON: The input wilson coefficients for the simulation software.\n",
    "\n",
    "* `number_bins`: The number of bins in each dimension of the simulation data\n",
    "\n",
    "* `train_amt`: How many of the MC dataset to used to train the model. This is usually ~70-80% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_path = 'nn_outputs/wilson_tau_csr_cvr'\n",
    "\n",
    "raw_path = 'raw_data/dstore_tau_csr_cvr'\n",
    "\n",
    "number_bins = 10\n",
    "\n",
    "number_of_files = len(glob.glob(raw_path + '/*.csv'))\n",
    "\n",
    "train_amount = int(0.6 * number_of_files)\n",
    "val_amount = int(0.2 * number_of_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalising Variables\n",
    "\n",
    "* Use set variables min-max normalise observables and construct 4d histograms. Then min-max normalise the bin-heights as well.\n",
    "\n",
    "* Only needs to be run once per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing File No. model_WET_tau_633: 100%|██████████| 1792/1792 [04:57<00:00,  6.03it/s] \n"
     ]
    }
   ],
   "source": [
    "du.file_to_normalise_4d(raw_path, normal_path, number_bins, bkg=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load in Data:\n",
    "\n",
    "1. Min-max normalise the bin-heights of the whole dataset\n",
    "\n",
    "2. Load in min-max normalised data, store them in appropriate variables and create dataloaders for model pass through.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Files: Bin Heights Already Normalised.\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_targets, val_test_inputs, val_test_targets, file_names = du.load_minmax_heights(normal_path, train_amount)\n",
    "\n",
    "# =-=-=-= Splitting the remaining data into validation and testing datasets =-=-=-=\n",
    "val_inputs, val_targets = val_test_inputs[:val_amount], val_test_targets[:val_amount]\n",
    "\n",
    "test_inputs, test_targets = val_test_inputs[val_amount:], val_test_targets[val_amount:]\n",
    "\n",
    "# =-=-=-= Load in the Bayesian Optimised Hyperparmaeters into a Dictionary =-=-=-=\n",
    "cfg =  nt.BO_CVAE(normal_path, train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(a). Create Data-Loaders with the above data, Ready for Model Passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-= DataLoader Initialisation =-=-=\n",
    "load_train = DataLoader(TensorDataset(train_inputs, train_targets), batch_size=cfg['batch_train'], shuffle=True, num_workers=0)\n",
    "\n",
    "load_val = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=cfg['batch_test'], shuffle=False, num_workers=0)\n",
    "\n",
    "load_test = DataLoader(TensorDataset(test_inputs, test_targets), batch_size=cfg['batch_test'], shuffle=False, num_workers=0)\n",
    "\n",
    "# =-=-= DataLoader Used for Model Output Storage Only =-=-=\n",
    "load_vis = DataLoader(TensorDataset(test_inputs, test_targets), batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model:\n",
    "\n",
    "1. Initialise loss function, Optimised-Model, and Optimiser.\n",
    "\n",
    "2. Initialise variables to store loss evolution and Early Stopping parameters.\n",
    "    - Loss evolution plotted later as a sanity check.\n",
    "\n",
    "    - Early Stopping: adjust only `early_stopping_patience` (integer) to adjust how strict training should be.\n",
    "\n",
    "3. Run the training loop - `train_epoch_early_CVAE` in the `CVAE_tools.py` notebook takes in:\n",
    "    - Model, Training dataloader, Validation dataloader, Loss Function, Optimiser, and operating DEVICE (GPU or CPU)\n",
    "\n",
    "    - This function takes care of all the loss calculation, backpropagation and optimiser steps.\n",
    "\n",
    "    - Returns Losses, which are stored and compared for early stopping.\n",
    "\n",
    "4. Store the Trained Model - Adjust the filename however you want - Variable: `save_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 63/150, Avg. Training Loss: 0.00111, Avg. Validation Loss: 0.00107:  41%|████▏     | 62/150 [00:28<00:40,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 63 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-= 1. Define the Loss Function and Model using Bayesian Optimised Hyperparameters =-=-=-=\n",
    "loss_func = nt.loss_function\n",
    "base = nt.CVAE_Opt(input_shape=cfg['dim'][0], output_shape=cfg['dim'][1], latent_dim=cfg['latent_dim'], encoder_layers=cfg['encoder'], decoder_layers=cfg['decoder'],\n",
    "                   dropout_enc=cfg['encoder_drop'], dropout_dec=cfg['decoder_drop'])\n",
    "base.to(DEVICE)\n",
    "\n",
    "# =-=-=-= 1. Optimiser specified =-=-=-=\n",
    "optimiser = torch.optim.AdamW(base.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# 2. Variables to store losses plotted out later\n",
    "loss_evol = []\n",
    "val_loss_evol = []\n",
    "\n",
    "# 2. Early Stopping Conditions - adjust only the patience value based on How constrained you want it.\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = np.inf\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 3. Training loops\n",
    "for epoch in (pbar := tqdm(range(cfg['epochs']))):\n",
    "    avg_loss, avg_val_loss = nt.train_epoch_early_CVAE(base, load_train, load_val, loss_func, optimiser, DEVICE)\n",
    "    pbar.set_description(f\"Training Epoch: {epoch + 1}/{cfg['epochs']}, Avg. Training Loss: {avg_loss:.5f}, Avg. Validation Loss: {avg_val_loss:.5f}\")\n",
    "\n",
    "    loss_evol.append(avg_loss)\n",
    "    val_loss_evol.append(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break\n",
    "\n",
    "# =-=-=-= 4. Change normal path where appropriate =-=-=-=-=\n",
    "save_path = normal_path + '/CVAE_tau_4d_sec.pth'\n",
    "torch.save(base.state_dict(), save_path)\n",
    "\n",
    "# plt.plot(list(range(cfg['epochs'])), loss_evol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(a). Training-Validation Loss Evolution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.rcParams[\"font.family\"] = \"Serif\"\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.plot(list(range(cfg['epochs']))[:len(loss_evol)], loss_evol, 'o-', color='blue', label='Training Loss')\n",
    "plt.plot(list(range(cfg['epochs']))[:len(loss_evol)], val_loss_evol, 'o-', color='red', label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=20, loc='right')\n",
    "plt.ylabel('Loss (Mean Squared Error)', fontsize=20, loc='top')\n",
    "plt.title('Model Loss Convergence in Training \\n Tau Decay; Wilson Coefficients cSR and cVR', fontsize=20)\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "plt.grid(which='minor', linestyle=':', linewidth=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the Model\n",
    "\n",
    "1. Initialising loss function, retrieving the trained model and loading it in.\n",
    "\n",
    "2. Run the testing function - evaluates the whole testing set and returns losses\n",
    "    - Good performance here should give values similar to the training loss\n",
    "\n",
    "3. Goes 1 by 1 - Using visualisation dataset to extract model outputs for each dataset \n",
    "    - Visualise Each dataset compared to EOS and directly verify performance.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss of Test Set 0.00106977991009545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [00:09, 36.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# =-=-= 1. Initialising of Testing Variables =-=-=\n",
    "loss_func = nt.loss_function\n",
    "best_model = nt.CVAE_Opt(input_shape=cfg['dim'][0], output_shape=cfg['dim'][1], latent_dim=cfg['latent_dim'], encoder_layers=cfg['encoder'], decoder_layers=cfg['decoder'],\n",
    "                   dropout_enc=cfg['encoder_drop'], dropout_dec=cfg['decoder_drop'])\n",
    "\n",
    "# =-=-= 1. Load best model data =-=-=\n",
    "best_model.load_state_dict(torch.load(save_path, weights_only = False))\n",
    "best_model.to(DEVICE)\n",
    "\n",
    "# =-=-= 2. Run the testing epoch - Operates like the training epoch without gradient calculation =-=-=\n",
    "out = nt.test_epoch_CVAE(best_model, load_test, loss_func, DEVICE)\n",
    "print('Average Loss of Test Set', out)\n",
    "\n",
    "# =-=-= 3. Store the model's outputs for the testing dataset =-=-=\n",
    "store_test = nt.test_visualise_4d_CVAE(best_model, load_vis, DEVICE, train_amount, normal_path, number_bins, file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(a). Plot out Integrated Histogram of One Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File number must exist in the model_outputs directory\n",
    "me.plot_1d_stk_tau(data_path=\"nn_outputs/wilson_tau_csr_cvr\", x_var='cos_theta_d', filenumber=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Evaluations\n",
    "\n",
    "**Finds Memory used, run-time, number of parameters, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(state_dict):\n",
    "    total_params = 0\n",
    "    for param_tensor in state_dict:\n",
    "        total_params += state_dict[param_tensor].numel()\n",
    "    return total_params\n",
    "\n",
    "# Load the state dictionary from the .pth file\n",
    "state_dict = torch.load(normal_path + '/CVAE_tau_4d.pth', weights_only=False)\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = count_parameters(state_dict)\n",
    "\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "\n",
    "fixed_inputs = torch.Tensor([0.5, 0.25]).to(DEVICE)\n",
    "csr_model_gen = nt.CVAE_Opt(input_shape=cfg['dim'][0], output_shape=cfg['dim'][1], latent_dim=cfg['latent_dim'], encoder_layers=cfg['encoder'], decoder_layers=cfg['decoder'],\n",
    "                   dropout_enc=cfg['encoder_drop'], dropout_dec=cfg['decoder_drop'])\n",
    "csr_model_gen.load_state_dict(torch.load('nn_outputs/wilson_tau_csr_cvr/CVAE_tau_4d.pth', weights_only=False))\n",
    "csr_model_gen.to(DEVICE)\n",
    "\n",
    "model_heights_list = []\n",
    "time_list = []\n",
    "memory_usage_list = []\n",
    "\n",
    "nbins=10\n",
    "\n",
    "bin_edges = np.linspace(0, 1, nbins + 1)\n",
    "bins = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "for i in range(50):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Measure memory usage before model generation\n",
    "        mem_before = process.memory_info().rss\n",
    "\n",
    "        sample = csr_model_gen.generate_histogram(fixed_inputs)\n",
    "\n",
    "        outputs = sample.to('cpu')\n",
    "        reshaped_outputs = np.array(outputs.reshape(nbins, nbins, nbins, nbins))\n",
    "\n",
    "        xpos, ypos, zpos, wpos = np.meshgrid(bins, bins, bins, bins, indexing=\"ij\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "                'q2': xpos.ravel(),\n",
    "                'cos_theta_l': ypos.ravel(),\n",
    "                'cos_theta_d': zpos.ravel(),\n",
    "                'phi': wpos.ravel(),\n",
    "                'bin_height': reshaped_outputs.ravel()\n",
    "            })\n",
    "        \n",
    "        df_integrated = result_df.groupby(['q2'])[\"bin_height\"].sum().reset_index()\n",
    "\n",
    "        x_unq = np.unique(result_df['q2'].values)\n",
    "        x_unq = (x_unq * 10.48) - 0.02\n",
    "\n",
    "        # Measure memory usage after model generation\n",
    "        mem_after = process.memory_info().rss\n",
    "\n",
    "        end_time = time.time()\n",
    "        sample = sample.cpu().numpy()\n",
    "\n",
    "        # Calculate memory usage for the model generation\n",
    "        mem_usage = mem_after - mem_before\n",
    "        memory_usage_list.append(mem_usage)\n",
    "\n",
    "        delta = end_time - start_time\n",
    "\n",
    "        time_list.append(delta)\n",
    "        model_heights_list.append(df_integrated['bin_height'].values)\n",
    "\n",
    "\n",
    "# Print memory usage statistics\n",
    "print(f\"Average memory usage per iteration: {np.mean(memory_usage_list) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Maximum memory usage in an iteration: {np.max(memory_usage_list) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Minimum memory usage in an iteration: {np.min(memory_usage_list) / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "\n",
    "fixed_inputs = torch.Tensor([0.5, 0.25]).to(DEVICE)\n",
    "csr_model_gen = nt.CVAE_Opt(input_shape=cfg['dim'][0], output_shape=cfg['dim'][1], latent_dim=cfg['latent_dim'], encoder_layers=cfg['encoder'], decoder_layers=cfg['decoder'],\n",
    "                   dropout_enc=cfg['encoder_drop'], dropout_dec=cfg['decoder_drop'])\n",
    "csr_model_gen.load_state_dict(torch.load('nn_outputs/wilson_tau_csr_cvr/CVAE_tau_4d.pth', weights_only=False))\n",
    "csr_model_gen.to(DEVICE)\n",
    "\n",
    "model_heights_list = []\n",
    "time_list = []\n",
    "memory_usage_list = []\n",
    "\n",
    "nbins=10\n",
    "\n",
    "bin_edges = np.linspace(0, 1, nbins + 1)\n",
    "bins = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "for i in range(50):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Measure memory usage before model generation\n",
    "        mem_before = process.memory_info().rss\n",
    "\n",
    "        sample = csr_model_gen.generate_histogram(fixed_inputs)\n",
    "\n",
    "        outputs = sample.to('cpu')\n",
    "        reshaped_outputs = np.array(outputs.reshape(nbins, nbins, nbins, nbins))\n",
    "\n",
    "        xpos, ypos, zpos, wpos = np.meshgrid(bins, bins, bins, bins, indexing=\"ij\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "                'q2': xpos.ravel(),\n",
    "                'cos_theta_l': ypos.ravel(),\n",
    "                'cos_theta_d': zpos.ravel(),\n",
    "                'phi': wpos.ravel(),\n",
    "                'bin_height': reshaped_outputs.ravel()\n",
    "            })\n",
    "        \n",
    "        df_integrated = result_df.groupby(['q2'])[\"bin_height\"].sum().reset_index()\n",
    "\n",
    "        x_unq = np.unique(result_df['q2'].values)\n",
    "        x_unq = (x_unq * 10.48) - 0.02\n",
    "\n",
    "        # Measure memory usage after model generation\n",
    "        mem_after = process.memory_info().rss\n",
    "\n",
    "        end_time = time.time()\n",
    "        sample = sample.cpu().numpy()\n",
    "\n",
    "        # Calculate memory usage for the model generation\n",
    "        mem_usage = mem_after - mem_before\n",
    "        memory_usage_list.append(mem_usage)\n",
    "\n",
    "        delta = end_time - start_time\n",
    "\n",
    "        time_list.append(delta)\n",
    "        model_heights_list.append(df_integrated['bin_height'].values)\n",
    "\n",
    "\n",
    "# Print memory usage statistics\n",
    "print(f\"Average memory usage per iteration: {np.mean(memory_usage_list) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Maximum memory usage in an iteration: {np.max(memory_usage_list) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Minimum memory usage in an iteration: {np.min(memory_usage_list) / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rough Work - Random Tests of Specific Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "w = np.diff(x_unq)[-1]\n",
    "\n",
    "for h in model_heights_list:\n",
    "    # Plot the NN dataset\n",
    "    ax.bar(x_unq, h, width=w, edgecolor='black', alpha=0.35, label='CVAE Output')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('q2', fontsize=20, loc='right')\n",
    "    ax.set_ylabel('Integrated Bin Heights', fontsize=20, loc='top')\n",
    "    ax.set_title(f'Model Generative Performance Comparison (Wilson Coefficient cSR: -5.0)', fontsize=20)\n",
    "\n",
    "    # Add minor ticks\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    ax.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_heights_list = []\n",
    "\n",
    "fp = os.listdir('nn_outputs/wilson_csr_fit/normal_inputs')\n",
    "\n",
    "for fil in fp:\n",
    "\n",
    "    f = os.path.join('nn_outputs/wilson_csr_fit/normal_inputs', fil)\n",
    "    with open(f, 'r') as file:\n",
    "        wilson = json.load(file)\n",
    "    \n",
    "    if wilson['wc_0'] == 0.75:\n",
    "        base_name = os.path.basename(f)\n",
    "        \n",
    "        data_name = base_name.replace('.json', '.csv')\n",
    "\n",
    "        data_path = os.path.join('nn_outputs/wilson_csr_fit/normal_targets', data_name)\n",
    "\n",
    "        df = pd.read_csv(data_path)\n",
    "        df_integrated = df.groupby(['q2'])[\"bin_height\"].sum().reset_index()\n",
    "\n",
    "        x_unq = np.unique(df['q2'].values)\n",
    "        x_unq = (x_unq * 10.48) - 0.02\n",
    "\n",
    "        eos_heights_list.append(df_integrated['bin_height'].values)\n",
    "    \n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "w = np.diff(x_unq)[-1]\n",
    "\n",
    "for h in eos_heights_list:\n",
    "    # Plot the NN dataset\n",
    "    ax.bar(x_unq, h, width=w, edgecolor='black', alpha=0.35, label='CVAE Output')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('q2', fontsize=20, loc='right')\n",
    "    ax.set_ylabel('Integrated Bin Heights', fontsize=20, loc='top')\n",
    "    ax.set_title(f'Model Generative Performance Comparison (Wilson Coefficient cSR: -5.0)', fontsize=20)\n",
    "\n",
    "    # Add minor ticks\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    ax.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_eos = np.array(eos_heights_list)\n",
    "single_column = np_eos[:, 4]\n",
    "\n",
    "mean_single_column = np.mean(single_column)\n",
    "std_single_column = np.std(single_column)\n",
    "\n",
    "adjusted = (single_column - mean_single_column)/std_single_column\n",
    "plt.hist(adjusted, bins=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
